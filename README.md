# Smart Energy Consumption Analytics using Big Data

## üìã Project Introduction

This project implements a comprehensive **Smart Energy Consumption Analytics system** using Big Data technologies to process **167 million+ IoT energy records** from 5,566 London households. The system delivers production-ready analytics with **99.87% forecasting accuracy** and real-time anomaly detection, providing actionable insights for energy optimization and grid management.

### üéØ Business Value & Objectives
- **Energy Industry Challenge**: Traditional energy systems lack real-time analytics for demand forecasting and anomaly detection
- **Big Data Opportunity**: 167M+ raw IoT records represent untapped potential for predictive analytics
- **Business Impact**: Potential **35% energy savings** through optimized consumption patterns and early anomaly detection
- **Technical Innovation**: Demonstrate scalable big data processing with PySpark, ML forecasting, and interactive visualization

### üìä Key Achievements
- **Predictive Analytics**: 99.87% accurate energy consumption forecasting (R¬≤ = 0.9987)
- **Anomaly Detection**: 0.07% anomaly rate identification with temporal clustering
- **Household Segmentation**: 4 distinct consumption patterns identified
- **Real-time Insights**: Complete 7-layer visualization dashboard for stakeholders
- **Scalable Architecture**: MongoDB-ready data exports for enterprise deployment

## üìÅ Project File Structure

### Root Directory Files
- `README.md` - This comprehensive project documentation
- `requirements.txt` - Python dependencies and versions
- `docker-compose.yml` - Docker container orchestration
- `Dockerfile` - Docker image configuration
- `PROJECT_SUMMARY.md` - Detailed project analysis and results
- `HADOOP_MAPREDUCE_ANALYSIS.md` - Hadoop MapReduce concepts explanation

### Scripts Directory (`scripts/`)
- `data_ingestion.py` - Raw data loading and Parquet conversion (167M+ records)
- `data_preprocessing.py` - Data cleaning, validation, and daily aggregation
- `feature_engineering.py` - 36+ ML features creation (lag, rolling, seasonal)
- `forecasting_model.py` - Linear Regression model training (99.87% accuracy)
- `anomaly_detection.py` - Hybrid K-means clustering + residual analysis
- `convert_to_csv.py` - CSV export functionality for MongoDB integration
- `merge_csv_for_mongodb.py` - MongoDB data preparation and merging

### Visualization Directory (`visualization/`)
- `streamlit_app.py` - Complete 7-layer interactive dashboard with MongoDB support

### Data Directory (`data/`)
- `Partitioned LCL Data/Small LCL Data/` - 168 CSV files with raw IoT data
- Processed Parquet files generated by pipeline:
  - `raw_energy_data.parquet` - Raw ingested data (167M+ records)
  - `daily_energy_data.parquet` - Preprocessed daily aggregations (1.93M records)
  - `energy_features.parquet` - ML-ready features (36+ features)
  - `forecasting_results.parquet` - Model predictions and metrics
  - `anomalies.parquet` - Detected anomalous records (1,279 records)

### Model Directory (`model/`)
- Trained ML models and performance metrics
- Model artifacts and evaluation results

### Automation Scripts
- `setup.ps1` - Local environment setup (Windows PowerShell)
- `run_pipeline.ps1` - Local pipeline execution (Windows PowerShell)
- `docker-setup.sh` / `docker-setup.ps1` - Docker environment preparation
- `run_docker_pipeline.sh` / `run_docker_pipeline.ps1` - Docker pipeline execution

## üìö Libraries & Dependencies

### Core Python Libraries (`requirements.txt`)
```txt
pandas>=1.5.0           # Data manipulation and analysis
numpy>=1.21.0           # Numerical computing
scikit-learn>=1.3.0     # Machine learning algorithms
matplotlib>=3.6.0       # Static plotting
seaborn>=0.12.0         # Statistical visualization
streamlit>=1.25.0       # Interactive web dashboard
plotly>=5.15.0          # Interactive visualizations
pyspark>=3.5.0          # Big data processing engine
openpyxl>=3.1.0         # Excel file handling
pymongo>=4.6.0          # MongoDB integration
python-dotenv>=1.0.0    # Environment variable management
```

### System Requirements
- **Python**: 3.7+ (Docker uses Python 3.x)
- **Java**: OpenJDK 17 (required for PySpark)
- **Memory**: Minimum 8GB RAM (16GB recommended)
- **Storage**: 10GB+ free space for data processing
- **OS**: Windows 10/11, Linux, or macOS

## üî¨ Methodologies & Algorithms Used

### 1. Data Processing Pipeline
- **Big Data Ingestion**: PySpark DataFrame API for distributed CSV processing
- **Data Validation**: Schema enforcement and missing value detection
- **Time Series Aggregation**: Half-hourly to daily consolidation with timezone handling
- **Memory Optimization**: Batch processing for 8GB RAM constraint

### 2. Feature Engineering Techniques
- **Lag Features**: 1-7 day consumption history (temporal dependencies)
- **Rolling Statistics**: 7-day mean, standard deviation, min/max (trend analysis)
- **Seasonal Encoding**: Sine/cosine transformations for cyclical patterns
- **Tariff Integration**: Peak/off-peak pricing feature engineering
- **Total Features**: 36+ engineered features for ML models

### 3. Machine Learning Algorithms
- **Forecasting Model**: Linear Regression with time-aware validation
  - **Accuracy**: R¬≤ = 0.9987 (99.87% prediction accuracy)
  - **Validation**: Time-based train/test split (prevents data leakage)
  - **Training Data**: 318,592 prediction instances
- **Anomaly Detection**: Hybrid K-means clustering + residual analysis
  - **Clustering**: K-means with k=5 for consumption pattern segmentation
  - **Detection**: Z-score > 3œÉ from cluster centroids
  - **Anomaly Rate**: 0.07% of total records (1,279 anomalies)

### 4. Big Data Processing Patterns
- **MapReduce Implementation**: PySpark operations mapping to Hadoop concepts
- **Distributed Processing**: 64-way parallelism for large-scale data
- **Fault Tolerance**: Automatic task retry and speculative execution
- **Data Partitioning**: Hash-based distribution for load balancing

### 5. Visualization & Analytics
- **Interactive Dashboard**: Streamlit + Plotly for 7-layer analytics
- **Real-time Updates**: Dynamic data loading from Parquet/MongoDB
- **Performance Optimization**: Lazy loading and caching for large datasets
- **User Experience**: Intuitive navigation with comprehensive insights

## üê≥ Docker Setup & Usage Guide

### Prerequisites for Docker
- **Docker Desktop**: Latest version installed and running
- **RAM Allocation**: Minimum 8GB allocated to Docker (16GB recommended)
- **Storage**: 10GB+ free disk space
- **System Resources**: Multi-core CPU for parallel processing

### Docker Environment Versions
- **Base Image**: `openjdk:17-slim` (Java 17)
- **Python**: 3.x (latest stable)
- **PySpark**: 3.5.0+
- **Spark**: 3.5.x (included with PySpark)
- **Hadoop**: 3.x (included with Spark)
- **MongoDB**: Ready for integration (external container recommended)

### Step 1: Docker Environment Setup

#### Windows PowerShell:
```powershell
# Make scripts executable
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Run Docker setup
.\docker-setup.ps1
```

#### Linux/macOS:
```bash
# Make scripts executable
chmod +x docker-setup.sh run_docker_pipeline.sh

# Run Docker setup
./docker-setup.sh
```

### Step 2: Build Docker Images

```bash
# Build the Docker images
docker-compose build

# Alternative: Build with no cache (if issues occur)
docker-compose build --no-cache
```

### Step 3: Start Docker Containers

```bash
# Start containers in detached mode
docker-compose up -d

# Verify containers are running
docker ps

# Expected output:
# CONTAINER ID   IMAGE          COMMAND                  CREATED         STATUS         PORTS                    NAMES
# abc123def456   energy-app     "tail -f /dev/null"      2 minutes ago   Up 2 minutes                            energy-analytics
# def456ghi789   energy-app     "streamlit run /app/‚Ä¶"   2 minutes ago   Up 2 minutes   0.0.0.0:8501->8501/tcp   energy-dashboard
```

### Step 4: Run Analytics Pipeline in Docker

#### Windows PowerShell:
```powershell
.\run_docker_pipeline.ps1
```

#### Linux/macOS:
```bash
./run_docker_pipeline.sh
```

### Step 5: Execute Individual Scripts (Manual Method)

If you need to run individual scripts manually (assuming docker pipeline file isn't given):

```bash
# Data Ingestion
docker exec energy-analytics python3 /app/scripts/data_ingestion.py

# Data Preprocessing
docker exec energy-analytics python3 /app/scripts/data_preprocessing.py

# Feature Engineering
docker exec energy-analytics python3 /app/scripts/feature_engineering.py

# Forecasting Model Training
docker exec energy-analytics python3 /app/scripts/forecasting_model.py

# Anomaly Detection
docker exec energy-analytics python3 /app/scripts/anomaly_detection.py

# CSV Export for MongoDB
docker exec energy-analytics python3 /app/scripts/convert_to_csv.py
```

### Step 6: Access Services

After successful pipeline execution:

- **üìä Streamlit Dashboard**: http://localhost:8501
- **‚ö° Spark Master UI**: http://localhost:8080 (if configured)
- **‚öôÔ∏è Spark Worker UI**: http://localhost:8081 (if configured)
- **üìì Jupyter Notebook**: http://localhost:8888 (if configured)

### Docker Management Commands

```bash
# View container logs
docker-compose logs -f

# View specific container logs
docker logs energy-analytics
docker logs energy-dashboard

# Stop containers
docker-compose stop

# Restart containers
docker-compose restart

# Shutdown and remove containers + volumes
docker-compose down -v

# Clean up Docker system
docker system prune -a --volumes
```

## üíª Local Setup (Alternative to Docker)

### Prerequisites
1. **Python 3.7+**: Download from https://python.org
2. **Java 8+**: Download from https://adoptium.net/
3. **PowerShell Execution**: Run `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`

### Local Setup Steps

1. **Setup Environment**
   ```powershell
   .\setup.ps1
   ```
   This installs PySpark and verifies prerequisites.

2. **Dataset Status: ‚úÖ Already Available!**
   - Dataset is already present in `data/Partitioned LCL Data/Small LCL Data/`
   - Contains 168 CSV files with ~167 million records
   - No download needed - proceed to next step

3. **Install Python Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run Data Pipeline**
   Execute the complete pipeline:
   ```powershell
   .\run_pipeline.ps1
   ```
   This automatically runs all scripts in order and processes your real dataset.

5. **Launch Visualization**
   ```bash
   streamlit run visualization/streamlit_app.py
   ```

## üìà Expected Performance Metrics

### Data Processing Results
- **Input Records**: 167,932,474 raw IoT readings
- **Processing Time**: ~114 minutes total (distributed across 5 stages)
- **Data Reduction**: 167M ‚Üí 318K ML-ready records (99.8% reduction)
- **Memory Usage**: Optimized for 8GB RAM systems

### Machine Learning Performance
- **Forecasting Accuracy**: R¬≤ = 0.9987 (99.87%)
- **MAE**: 0.0381 kWh per prediction
- **RMSE**: 0.3733 kWh per prediction
- **Anomaly Detection Rate**: 0.07% (1,279 anomalies identified)

### System Performance
- **Parallelism**: 64-way distributed processing
- **Fault Tolerance**: Automatic retry and recovery
- **Scalability**: Linear scaling with additional nodes
- **Resource Efficiency**: Optimized for cloud and on-premises deployment

## üîß Troubleshooting

### Common Docker Issues
```bash
# If containers fail to start
docker-compose down -v
docker system prune -a --volumes
docker-compose up -d

# If port 8501 is already in use
docker-compose down
docker-compose up -d

# Check container resource usage
docker stats
```

### Common PySpark Issues
- **Java Version**: Ensure Java 8+ is installed and JAVA_HOME is set
- **Memory Issues**: Increase Docker RAM allocation to 16GB
- **Permission Issues**: Run Docker commands as administrator/sudo

### Data Pipeline Issues
- **Missing Files**: Ensure all CSV files are in `data/` directory
- **Out of Memory**: Reduce `spark.sql.shuffle.partitions` in scripts
- **Slow Processing**: Increase Docker CPU allocation

## üìû Support & Documentation

### Additional Resources
- `PROJECT_SUMMARY.md` - Detailed technical analysis and results
- `HADOOP_MAPREDUCE_ANALYSIS.md` - Hadoop concepts and MapReduce patterns
- `scripts/mapreduce_demo.py` - Pure Python MapReduce demonstration

### Getting Help
1. Check the troubleshooting section above
2. Review container logs: `docker-compose logs -f`
3. Verify system requirements are met
4. Check GitHub issues for similar problems

---

## üöÄ Quick Start Summary

**For Docker (Recommended):**
```bash
# Setup
.\docker-setup.ps1
docker-compose build
docker-compose up -d

# Run pipeline
.\run_docker_pipeline.ps1

# Access dashboard
# http://localhost:8501
```

**For Local Development:**
```powershell
.\setup.ps1
pip install -r requirements.txt
.\run_pipeline.ps1
streamlit run visualization/streamlit_app.py
```

**Project completed successfully with 99.87% forecasting accuracy! üéâ**